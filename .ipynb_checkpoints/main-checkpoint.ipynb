{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "from model import Clusterizer\n",
    "from utils import use_cuda, visualize_cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_gaussian_mixture_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "batch_size = 32\n",
    "n_clusters = 4\n",
    "dataset_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, means, covs = generate_gaussian_mixture_dataset(input_size=input_size,\n",
    "                                                         n_clusters=n_clusters,\n",
    "                                                         dataset_size=dataset_size)\n",
    "\n",
    "dataloader = {x: DataLoader(dataset[x], batch_size=batch_size, shuffle=True) for x in dataset.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating and training the model until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ----- Loss : 489.98\n",
      "Epoch : 1 ----- Loss : 262.06\n",
      "Epoch : 2 ----- Loss : 142.06\n",
      "Epoch : 3 ----- Loss : 106.02\n",
      "Epoch : 4 ----- Loss : 105.33\n",
      "Epoch : 5 ----- Loss : 104.99\n",
      "Epoch : 6 ----- Loss : 104.61\n",
      "Epoch : 7 ----- Loss : 104.29\n",
      "Epoch : 8 ----- Loss : 104.09\n",
      "Epoch : 9 ----- Loss : 103.96\n",
      "Epoch : 10 ----- Loss : 103.80\n",
      "Epoch : 11 ----- Loss : 103.72\n",
      "Epoch : 12 ----- Loss : 103.57\n",
      "Epoch : 13 ----- Loss : 103.37\n",
      "Epoch : 14 ----- Loss : 103.16\n",
      "Epoch : 15 ----- Loss : 102.88\n",
      "Epoch : 16 ----- Loss : 102.70\n",
      "Epoch : 17 ----- Loss : 102.24\n",
      "Epoch : 18 ----- Loss : 101.91\n",
      "Epoch : 19 ----- Loss : 101.59\n",
      "Epoch : 20 ----- Loss : 101.33\n",
      "Epoch : 21 ----- Loss : 100.99\n",
      "Epoch : 22 ----- Loss : 100.81\n",
      "Epoch : 23 ----- Loss : 100.59\n",
      "Epoch : 24 ----- Loss : 100.35\n",
      "Epoch : 25 ----- Loss : 100.13\n",
      "Epoch : 26 ----- Loss : 99.96\n",
      "Epoch : 27 ----- Loss : 99.75\n",
      "Epoch : 28 ----- Loss : 99.56\n",
      "Epoch : 29 ----- Loss : 99.41\n",
      "Epoch : 30 ----- Loss : 99.22\n",
      "Epoch : 31 ----- Loss : 99.06\n",
      "Epoch : 32 ----- Loss : 98.92\n",
      "Epoch : 33 ----- Loss : 98.74\n",
      "Epoch : 34 ----- Loss : 98.62\n",
      "Epoch : 35 ----- Loss : 98.43\n",
      "Epoch : 36 ----- Loss : 98.25\n",
      "Epoch : 37 ----- Loss : 98.07\n",
      "Epoch : 38 ----- Loss : 97.97\n",
      "Epoch : 39 ----- Loss : 97.78\n",
      "Epoch : 40 ----- Loss : 97.59\n",
      "Epoch : 41 ----- Loss : 97.39\n",
      "Epoch : 42 ----- Loss : 97.22\n",
      "Epoch : 43 ----- Loss : 97.06\n",
      "Epoch : 44 ----- Loss : 96.88\n",
      "Epoch : 45 ----- Loss : 96.64\n",
      "Epoch : 46 ----- Loss : 96.50\n",
      "Epoch : 47 ----- Loss : 96.32\n",
      "Epoch : 48 ----- Loss : 96.12\n",
      "Epoch : 49 ----- Loss : 95.96\n",
      "Epoch : 50 ----- Loss : 95.84\n",
      "Epoch : 51 ----- Loss : 95.67\n",
      "Epoch : 52 ----- Loss : 95.56\n",
      "Epoch : 53 ----- Loss : 95.41\n",
      "Epoch : 54 ----- Loss : 95.28\n",
      "Epoch : 55 ----- Loss : 95.19\n",
      "Epoch : 56 ----- Loss : 95.03\n",
      "Epoch : 57 ----- Loss : 94.91\n",
      "Epoch : 58 ----- Loss : 94.81\n",
      "Epoch : 59 ----- Loss : 94.73\n",
      "Epoch : 60 ----- Loss : 94.62\n",
      "Epoch : 61 ----- Loss : 94.48\n",
      "Epoch : 62 ----- Loss : 94.39\n",
      "Epoch : 63 ----- Loss : 94.29\n",
      "Epoch : 64 ----- Loss : 94.19\n",
      "Epoch : 65 ----- Loss : 94.11\n",
      "Epoch : 66 ----- Loss : 94.00\n",
      "Epoch : 67 ----- Loss : 93.89\n",
      "Epoch : 68 ----- Loss : 93.78\n",
      "Epoch : 69 ----- Loss : 93.66\n",
      "Epoch : 70 ----- Loss : 93.62\n",
      "Epoch : 71 ----- Loss : 93.50\n",
      "Epoch : 72 ----- Loss : 93.42\n",
      "Epoch : 73 ----- Loss : 93.28\n",
      "Epoch : 74 ----- Loss : 93.24\n",
      "Epoch : 75 ----- Loss : 93.12\n",
      "Epoch : 76 ----- Loss : 92.98\n",
      "Epoch : 77 ----- Loss : 92.86\n",
      "Epoch : 78 ----- Loss : 92.79\n",
      "Epoch : 79 ----- Loss : 92.70\n",
      "Epoch : 80 ----- Loss : 92.61\n",
      "Epoch : 81 ----- Loss : 92.53\n",
      "Epoch : 82 ----- Loss : 92.39\n",
      "Epoch : 83 ----- Loss : 92.32\n",
      "Epoch : 84 ----- Loss : 92.25\n",
      "Epoch : 85 ----- Loss : 92.09\n",
      "Epoch : 86 ----- Loss : 92.06\n",
      "Epoch : 87 ----- Loss : 91.98\n",
      "Epoch : 88 ----- Loss : 91.87\n",
      "Epoch : 89 ----- Loss : 91.78\n",
      "Epoch : 90 ----- Loss : 91.79\n"
     ]
    }
   ],
   "source": [
    "model = Clusterizer(input_size=input_size, n_clusters=n_clusters)\n",
    "loss = train(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing results\n",
    "Here, we project the data in 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cluster_assignment(model, dataset, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Moons Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_two_moons_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "batch_size = 32\n",
    "n_clusters = 2\n",
    "dataset_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_two_moons_dataset(dataset_size)\n",
    "dataloader = {x: DataLoader(dataset[x], batch_size=batch_size, shuffle=True) for x in dataset.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating and training the model until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Clusterizer(input_size=input_size, n_clusters=n_clusters)\n",
    "loss = train(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cluster_assignment(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
